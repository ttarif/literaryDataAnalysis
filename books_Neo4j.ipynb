{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Data Analysis and Neo4j Integration\n",
    "\n",
    "## Author: Yousuf Rajput, Tarek Tarif\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter notebook performs an analysis on book-related data and integrates it with a Neo4j graph database. The analysis includes consolidating and reordering book data, reading and processing JSON and CSV files, and creating relationships between various entities like Books, Authors, Ratings, Languages, and Reviews.\n",
    "\n",
    "## Code Sections\n",
    "\n",
    "1. **Consolidate and Reorder CSV Data:**\n",
    "   - Reads and consolidates book data from multiple CSV files.\n",
    "   - Reorders the data to ensure a common structure.\n",
    "\n",
    "2. **JSON to CSV Conversion:**\n",
    "   - Reads a JSON file in chunks and converts it to a CSV file.\n",
    "\n",
    "3. **Remove Columns from CSV:**\n",
    "   - Removes specified columns from a CSV file.\n",
    "\n",
    "4. **Extract Common ISBNs and Save to new CSV file:**\n",
    "   - Finds common ISBNS between 2 datasets and saves to CSV file.\n",
    "\n",
    "5. **Correct CSV files:**\n",
    "   - Correct syntax errors in CSV files for proper instertion.\n",
    "\n",
    "6. **Text Extraction from 1000 Books Page:**\n",
    "   - Scrapes text content for Book titles and saves it to a text file.\n",
    "\n",
    "7. **Read Wikipedia Tables:**\n",
    "   - Reads tables from a Wikipedia page and saves the 'Book' column to text files.\n",
    "\n",
    "8. **Neo4j Database Integration:**\n",
    "   - Connects to a Neo4j database.\n",
    "   - Inserts structured and semi-structured data into the database.\n",
    "   - Creates relationships between nodes (Books, Ratings, Languages, Authors, Reviews).\n",
    "\n",
    "9. **Neo4j Queries for Analysis:**\n",
    "   - Executes 5 complex Neo4j queries.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Ensure Neo4j is installed and a database is running.\n",
    "- Python libraries required: pandas, requests, beautifulsoup4, py2neo.\n",
    "\n",
    "### Required Data:\n",
    "- Semi-Structured Dataset Download Link: https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/goodreads_books.json.gz\n",
    "- Structured Dataset Download Link: https://www.kaggle.com/datasets/bahramjannesarr/goodreads-book-datasets-10m/data\n",
    "\n",
    "**Note:** Update Neo4j connection details and file paths as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from py2neo import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Consolidate and Reorder CSV Files\n",
    "\n",
    "This step defines a function `consolidate_and_reorder` that consolidates and reorders data from multiple CSV files into a single CSV file. It uses the pandas library for data manipulation.\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "def consolidate_and_reorder(book_files, output_file):\n",
    "    \"\"\"\n",
    "    Consolidate and reorder data from multiple CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - book_files (list): List of CSV file paths to consolidate.\n",
    "    - output_file (str): Output file path to save the consolidated data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Columns [] are missing in file 'archive/book100k-200k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book200k-300k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book300k-400k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book400k-500k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book500k-600k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book600k-700k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book700k-800k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book800k-900k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book900k-1000k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book1000k-1100k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book1100k-1200k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book1200k-1300k.csv' and will be dropped.\n",
      "Warning: Columns [] are missing in file 'archive/book1300k-1400k.csv' and will be dropped.\n",
      "Consolidated data saved to 'books.csv'.\n"
     ]
    }
   ],
   "source": [
    "def consolidate_and_reorder(book_files, output_file):\n",
    "    common_header = None\n",
    "    consolidated_data = []\n",
    "\n",
    "    for file_path in book_files:\n",
    "        # Read the first row (header) of the CSV file\n",
    "        header = pd.read_csv(file_path, nrows=0).columns.tolist()\n",
    "\n",
    "        # Set as common header if not set yet\n",
    "        if common_header is None:\n",
    "            common_header = header\n",
    "        else:\n",
    "            # Ensure that the current header matches the common header\n",
    "            if header != common_header:\n",
    "                # Find columns that are present in the common header but not in the current header\n",
    "                missing_columns_common = [col for col in common_header if col not in header]\n",
    "\n",
    "                # Find columns that are present in the current header but not in the common header\n",
    "                missing_columns_current = [col for col in header if col not in common_header]\n",
    "\n",
    "                # Drop the missing columns from both the common header and the current header\n",
    "                common_header = [col for col in common_header if col not in missing_columns_common]\n",
    "                header = [col for col in header if col not in missing_columns_current]\n",
    "\n",
    "                # Warn user about the missing columns\n",
    "                print(f\"Warning: Columns {missing_columns_common} are missing in file '{file_path}' and will be dropped.\")\n",
    "\n",
    "        # Read the entire CSV file and append to consolidated_data\n",
    "        data = pd.read_csv(file_path, usecols=common_header)\n",
    "        consolidated_data.append(data)\n",
    "\n",
    "    # Concatenate all data frames\n",
    "    consolidated_df = pd.concat(consolidated_data, ignore_index=True)\n",
    "\n",
    "    # Reorder columns to match the common header\n",
    "    consolidated_df = consolidated_df[common_header]\n",
    "\n",
    "    # Save the consolidated and reordered data to 'books.csv'\n",
    "    consolidated_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Input book files\n",
    "book_files = [\n",
    "    'archive/book1-100k.csv',\n",
    "    'archive/book100k-200k.csv',\n",
    "    'archive/book200k-300k.csv',\n",
    "    'archive/book300k-400k.csv',\n",
    "    'archive/book400k-500k.csv',\n",
    "    'archive/book500k-600k.csv',\n",
    "    'archive/book600k-700k.csv',\n",
    "    'archive/book700k-800k.csv',\n",
    "    'archive/book800k-900k.csv',\n",
    "    'archive/book900k-1000k.csv',\n",
    "    'archive/book1000k-1100k.csv',\n",
    "    'archive/book1100k-1200k.csv',\n",
    "    'archive/book1200k-1300k.csv',\n",
    "    'archive/book1300k-1400k.csv'\n",
    "]\n",
    "\n",
    "# Output file\n",
    "output_file = 'books.csv'\n",
    "\n",
    "# Consolidate and reorder the data\n",
    "consolidate_and_reorder(book_files, output_file)\n",
    "\n",
    "print(f\"Consolidated data saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Read JSON File in Chunks and Save to CSV\n",
    "\n",
    "The function `count_entries` counts the number of entries in a JSON file. It reads the JSON file and uses the `json.load` function to parse the data. The count of entries is then returned. The JSON file is then loaded in chunks to a csv file for faster processing. \n",
    "\n",
    "\n",
    "### Function Signature\n",
    "```python\n",
    "def count_entries(json_file):\n",
    "    \"\"\"\n",
    "    Count the number of entries in a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - json_file (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    int: Number of entries in the JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON file has 2360655 entries.\n"
     ]
    }
   ],
   "source": [
    "def count_entries(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        entries = 0\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Try to load each line as a JSON object\n",
    "                json_object = json.loads(line)\n",
    "                entries += 1\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle invalid JSON on a line (skip or log as needed)\n",
    "                print(f\"Skipping invalid JSON on line: {line.strip()}\")\n",
    "    return entries\n",
    "\n",
    "json_file = 'proj/Data/Semi-Structured/OLD/goodreads_books.json'  # replace with your file path\n",
    "num_entries = count_entries(json_file)\n",
    "print(f'The JSON file has {num_entries} entries.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 chunks\n",
      "Processed 2 chunks\n",
      "Processed 3 chunks\n",
      "Processed 4 chunks\n",
      "Processed 5 chunks\n",
      "Processed 6 chunks\n",
      "Processed 7 chunks\n",
      "Processed 8 chunks\n",
      "Processed 9 chunks\n",
      "Processed 10 chunks\n",
      "Processed 11 chunks\n",
      "Processed 12 chunks\n",
      "Processed 13 chunks\n",
      "Processed 14 chunks\n",
      "Processed 15 chunks\n",
      "Processed 16 chunks\n",
      "Processed 17 chunks\n",
      "Processed 18 chunks\n",
      "Processed 19 chunks\n",
      "Processed 20 chunks\n",
      "Processed 21 chunks\n",
      "Processed 22 chunks\n",
      "Processed 23 chunks\n",
      "Processed 24 chunks\n",
      "Processed 25 chunks\n",
      "Processed 26 chunks\n",
      "Processed 27 chunks\n",
      "Processed 28 chunks\n",
      "Processed 29 chunks\n",
      "Processed 30 chunks\n",
      "Processed 31 chunks\n",
      "Processed 32 chunks\n",
      "Processed 33 chunks\n",
      "Processed 34 chunks\n",
      "Processed 35 chunks\n",
      "Processed 36 chunks\n",
      "Processed 37 chunks\n",
      "Processed 38 chunks\n",
      "Processed 39 chunks\n",
      "Processed 40 chunks\n",
      "Processed 41 chunks\n",
      "Processed 42 chunks\n",
      "Processed 43 chunks\n",
      "Processed 44 chunks\n",
      "Processed 45 chunks\n",
      "Processed 46 chunks\n",
      "Processed 47 chunks\n",
      "Processed 48 chunks\n",
      "Processed 49 chunks\n",
      "Processed 50 chunks\n",
      "Processed 51 chunks\n",
      "Processed 52 chunks\n",
      "Processed 53 chunks\n",
      "Processed 54 chunks\n",
      "Processed 55 chunks\n",
      "Processed 56 chunks\n",
      "Processed 57 chunks\n",
      "Processed 58 chunks\n",
      "Processed 59 chunks\n",
      "Processed 60 chunks\n",
      "Processed 61 chunks\n",
      "Processed 62 chunks\n",
      "Processed 63 chunks\n",
      "Processed 64 chunks\n",
      "Processed 65 chunks\n",
      "Processed 66 chunks\n",
      "Processed 67 chunks\n",
      "Processed 68 chunks\n",
      "Processed 69 chunks\n",
      "Processed 70 chunks\n",
      "Processed 71 chunks\n",
      "Processed 72 chunks\n",
      "Processed 73 chunks\n",
      "Processed 74 chunks\n",
      "Processed 75 chunks\n",
      "Processed 76 chunks\n",
      "Processed 77 chunks\n",
      "Processed 78 chunks\n",
      "Processed 79 chunks\n",
      "Processed 80 chunks\n",
      "Processed 81 chunks\n",
      "Processed 82 chunks\n",
      "Processed 83 chunks\n",
      "Processed 84 chunks\n",
      "Processed 85 chunks\n",
      "Processed 86 chunks\n",
      "Processed 87 chunks\n",
      "Processed 88 chunks\n",
      "Processed 89 chunks\n",
      "Processed 90 chunks\n",
      "Processed 91 chunks\n",
      "Processed 92 chunks\n",
      "Processed 93 chunks\n",
      "Processed 94 chunks\n",
      "Processed 95 chunks\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file in chunks\n",
    "json_reader = pd.read_json(json_file, lines=True, chunksize=25000)\n",
    "\n",
    "# Open the CSV file\n",
    "with open('goodreads_books.csv', 'w', encoding='utf-8') as f:\n",
    "    # Initialize the chunk counter\n",
    "    chunk_counter = 0\n",
    "\n",
    "    # Process each chunk\n",
    "    for chunk in json_reader:\n",
    "        # Increment the chunk counter\n",
    "        chunk_counter += 1\n",
    "\n",
    "        # Write the chunk to the CSV file\n",
    "        chunk.to_csv(f, index=False, header=f.tell()==0)\n",
    "\n",
    "        # Print the chunk counter\n",
    "        print(f'Processed {chunk_counter} chunks')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 3: Remove Specified Columns from CSV File\n",
    "\n",
    "Columns that are not needed are removed from the CSV file (`goodreads_books.csv`) and the updated data is saved to a new CSV file (`goodreads_books_updated.csv`).\n",
    "\n",
    "### Steps:\n",
    "1. Define a list of columns to be removed (`columns_to_remove`).\n",
    "2. Create a new CSV file (`goodreads_books_updated.csv`) to save the updated data.\n",
    "3. Read the original CSV file (`goodreads_books.csv`) in chunks to manage memory efficiently.\n",
    "4. Iterate through each chunk, remove specified columns, and append the updated chunk to the new CSV file.\n",
    "5. Print progress information during chunk processing.\n",
    "6. Display 'Done!' once the entire CSV file has been processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Processing chunk 73...\n",
      "Processing chunk 74...\n",
      "Processing chunk 75...\n",
      "Processing chunk 76...\n",
      "Processing chunk 77...\n",
      "Processing chunk 78...\n",
      "Processing chunk 79...\n",
      "Processing chunk 80...\n",
      "Processing chunk 81...\n",
      "Processing chunk 82...\n",
      "Processing chunk 83...\n",
      "Processing chunk 84...\n",
      "Processing chunk 85...\n",
      "Processing chunk 86...\n",
      "Processing chunk 87...\n",
      "Processing chunk 88...\n",
      "Processing chunk 89...\n",
      "Processing chunk 90...\n",
      "Processing chunk 91...\n",
      "Processing chunk 92...\n",
      "Processing chunk 93...\n",
      "Processing chunk 94...\n",
      "Processing chunk 95...\n",
      "Processing chunk 96...\n",
      "Processing chunk 97...\n",
      "Processing chunk 98...\n",
      "Processing chunk 99...\n",
      "Processing chunk 100...\n",
      "Processing chunk 101...\n",
      "Processing chunk 102...\n",
      "Processing chunk 103...\n",
      "Processing chunk 104...\n",
      "Processing chunk 105...\n",
      "Processing chunk 106...\n",
      "Processing chunk 107...\n",
      "Processing chunk 108...\n",
      "Processing chunk 109...\n",
      "Processing chunk 110...\n",
      "Processing chunk 111...\n",
      "Processing chunk 112...\n",
      "Processing chunk 113...\n",
      "Processing chunk 114...\n",
      "Processing chunk 115...\n",
      "Processing chunk 116...\n",
      "Processing chunk 117...\n",
      "Processing chunk 118...\n",
      "Processing chunk 119...\n",
      "Processing chunk 120...\n",
      "Processing chunk 121...\n",
      "Processing chunk 122...\n",
      "Processing chunk 123...\n",
      "Processing chunk 124...\n",
      "Processing chunk 125...\n",
      "Processing chunk 126...\n",
      "Processing chunk 127...\n",
      "Processing chunk 128...\n",
      "Processing chunk 129...\n",
      "Processing chunk 130...\n",
      "Processing chunk 131...\n",
      "Processing chunk 132...\n",
      "Processing chunk 133...\n",
      "Processing chunk 134...\n",
      "Processing chunk 135...\n",
      "Processing chunk 136...\n",
      "Processing chunk 137...\n",
      "Processing chunk 138...\n",
      "Processing chunk 139...\n",
      "Processing chunk 140...\n",
      "Processing chunk 141...\n",
      "Processing chunk 142...\n",
      "Processing chunk 143...\n",
      "Processing chunk 144...\n",
      "Processing chunk 145...\n",
      "Processing chunk 146...\n",
      "Processing chunk 147...\n",
      "Processing chunk 148...\n",
      "Processing chunk 149...\n",
      "Processing chunk 150...\n",
      "Processing chunk 151...\n",
      "Processing chunk 152...\n",
      "Processing chunk 153...\n",
      "Processing chunk 154...\n",
      "Processing chunk 155...\n",
      "Processing chunk 156...\n",
      "Processing chunk 157...\n",
      "Processing chunk 158...\n",
      "Processing chunk 159...\n",
      "Processing chunk 160...\n",
      "Processing chunk 161...\n",
      "Processing chunk 162...\n",
      "Processing chunk 163...\n",
      "Processing chunk 164...\n",
      "Processing chunk 165...\n",
      "Processing chunk 166...\n",
      "Processing chunk 167...\n",
      "Processing chunk 168...\n",
      "Processing chunk 169...\n",
      "Processing chunk 170...\n",
      "Processing chunk 171...\n",
      "Processing chunk 172...\n",
      "Processing chunk 173...\n",
      "Processing chunk 174...\n",
      "Processing chunk 175...\n",
      "Processing chunk 176...\n",
      "Processing chunk 177...\n",
      "Processing chunk 178...\n",
      "Processing chunk 179...\n",
      "Processing chunk 180...\n",
      "Processing chunk 181...\n",
      "Processing chunk 182...\n",
      "Processing chunk 183...\n",
      "Processing chunk 184...\n",
      "Processing chunk 185...\n",
      "Processing chunk 186...\n",
      "Processing chunk 187...\n",
      "Processing chunk 188...\n",
      "Processing chunk 189...\n",
      "Processing chunk 190...\n",
      "Processing chunk 191...\n",
      "Processing chunk 192...\n",
      "Processing chunk 193...\n",
      "Processing chunk 194...\n",
      "Processing chunk 195...\n",
      "Processing chunk 196...\n",
      "Processing chunk 197...\n",
      "Processing chunk 198...\n",
      "Processing chunk 199...\n",
      "Processing chunk 200...\n",
      "Processing chunk 201...\n",
      "Processing chunk 202...\n",
      "Processing chunk 203...\n",
      "Processing chunk 204...\n",
      "Processing chunk 205...\n",
      "Processing chunk 206...\n",
      "Processing chunk 207...\n",
      "Processing chunk 208...\n",
      "Processing chunk 209...\n",
      "Processing chunk 210...\n",
      "Processing chunk 211...\n",
      "Processing chunk 212...\n",
      "Processing chunk 213...\n",
      "Processing chunk 214...\n",
      "Processing chunk 215...\n",
      "Processing chunk 216...\n",
      "Processing chunk 217...\n",
      "Processing chunk 218...\n",
      "Processing chunk 219...\n",
      "Processing chunk 220...\n",
      "Processing chunk 221...\n",
      "Processing chunk 222...\n",
      "Processing chunk 223...\n",
      "Processing chunk 224...\n",
      "Processing chunk 225...\n",
      "Processing chunk 226...\n",
      "Processing chunk 227...\n",
      "Processing chunk 228...\n",
      "Processing chunk 229...\n",
      "Processing chunk 230...\n",
      "Processing chunk 231...\n",
      "Processing chunk 232...\n",
      "Processing chunk 233...\n",
      "Processing chunk 234...\n",
      "Processing chunk 235...\n",
      "Processing chunk 236...\n",
      "Processing chunk 237...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_remove = ['text_reviews_count', 'series', 'description', 'link', 'num_pages', 'isbn13', 'url', 'authors', 'image_url']\n",
    "\n",
    "# Create a new CSV file to save the updated data\n",
    "with open('goodreads_books_updated.csv', 'w') as f:\n",
    "    pass\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunksize = 10 ** 4  # adjust this value depending on performance\n",
    "chunk_counter = 0\n",
    "for chunk in pd.read_csv('goodreads_books.csv', chunksize=chunksize):\n",
    "    chunk_counter += 1\n",
    "    print(f'Processing chunk {chunk_counter}...')\n",
    "    # Remove the columns\n",
    "    chunk = chunk.drop(columns_to_remove, axis=1)\n",
    "    # Append the updated chunk to the new CSV file\n",
    "    chunk.to_csv('goodreads_books_updated.csv', mode='a', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 4: Extract Common ISBNs and Save to a New CSV File\n",
    "\n",
    "Extract common ISBNs from the 'goodreads_books_updated.csv' file that match those in the 'books.csv' file. The resulting data is then saved to a new CSV file named 'goodreads_books_common.csv'. This process helps filter and retain only the entries with common ISBNs.\n",
    "\n",
    "### Steps:\n",
    "1. Read the 'books.csv' file and store the unique ISBNs in a set.\n",
    "2. Create a new CSV file ('goodreads_books_common.csv') to save the updated data.\n",
    "3. Read the 'goodreads_books_updated.csv' file in chunks for efficient memory usage.\n",
    "4. Remove rows with 'null' ISBN values from each chunk.\n",
    "5. Identify common ISBNs between the chunk and the 'books.csv' file.\n",
    "6. Filter the chunk to include only rows with common ISBNs.\n",
    "7. Append the updated chunk to the new CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# List of common ISBNs\n",
    "common_isbns = []\n",
    "\n",
    "# Read the 'books.csv' file and store the ISBNs\n",
    "df_books = pd.read_csv('books.csv')\n",
    "book_isbns = set(df_books['ISBN'])\n",
    "\n",
    "# Create a new CSV file to save the updated data\n",
    "with open('goodreads_books_common.csv', 'w') as f:\n",
    "    pass\n",
    "\n",
    "chunksize = 10 ** 4  # adjust this value depending on performance\n",
    "for chunk in pd.read_csv('goodreads_books_updated.csv', chunksize=chunksize):\n",
    "    # Remove rows with 'null' ISBN/title values\n",
    "    chunk = chunk[~chunk['isbn'].isnull()]\n",
    "    chunk = chunk[~chunk['title'].isnull()]\n",
    "    # Find the common ISBNs\n",
    "    common_isbns_chunk = pd.Series(list(set(chunk['isbn']).intersection(book_isbns)))\n",
    "    common_isbns.extend(common_isbns_chunk)\n",
    "\n",
    "    # Filter the chunk to only include rows with common ISBNs\n",
    "    chunk = chunk[chunk['isbn'].isin(common_isbns_chunk)]\n",
    "\n",
    "    # Append the updated chunk to the new CSV file\n",
    "    chunk.to_csv('goodreads_books_common.csv', mode='a', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Correct CSV File with Quotes\n",
    "\n",
    "Takes input a CSV file named 'goodreads_books_common.csv', corrects potential problems with quotes in the data, and saves the corrected data to a new CSV file named 'fixed_goodreads_books_common.csv'.\n",
    "\n",
    "### Steps:\n",
    "1. Define a function `escape_quotes` that uses `ast.literal_eval` to handle escaping quotes in a given value.\n",
    "2. Specify input and output file paths.\n",
    "3. Open the input CSV file in read mode and the output CSV file in write mode with newline='' to prevent extra newline characters.\n",
    "4. Initialize CSV readers and writers.\n",
    "5. Iterate through rows in the input CSV file.\n",
    "6. Apply the `escape_quotes` function to all values in each row.\n",
    "7. Write the corrected row to the output CSV file.\n",
    "8. Print a message indicating the completion of the correction process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction complete. Fixed data written to goodreads_books_common.csv\n"
     ]
    }
   ],
   "source": [
    "def escape_quotes(value):\n",
    "    try:\n",
    "        return ast.literal_eval(f'\"{value}\"')\n",
    "    except (SyntaxError, ValueError):\n",
    "        return value\n",
    "\n",
    "input_file = 'goodreads_books_common.csv'\n",
    "output_file = 'fixed_goodreads_books_common.csv'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    # Check if the CSV file has a header\n",
    "    fieldnames = reader.fieldnames if reader.fieldnames is not None else ['column1', 'column2', 'column3']  # Replace with actual column names\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    # Write the header only if it exists\n",
    "    if fieldnames is not None:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        # Apply the escape_quotes function to all values in the row\n",
    "        row = {key: escape_quotes(value) for key, value in row.items()}\n",
    "        # Write the corrected row to the output file\n",
    "        writer.writerow(row)\n",
    "        \n",
    "os.rename(output_file, input_file)\n",
    "print(f\"Correction complete. Fixed data written to {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load CSV, Get Sample, and Save\n",
    "df = pd.read_csv('goodreads_books_common.csv')\n",
    "df = df.head(100)\n",
    "df.to_csv('goodreads_common_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Extract Titles from Webpage and Write to Text File\n",
    "\n",
    "This code retrieves a list of book titles from a specific webpage using a provided URL. It utilizes BeautifulSoup and CSS selectors to locate the relevant HTML element containing the book titles. The extracted titles are then processed and written to a text file named '1000books.txt'. If the specified HTML element is not found, an appropriate message is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# URL of the webpage you want to extract\n",
    "url = \"https://libraryof1000books.wordpress.com/the-list-of-1000-books/\"\n",
    "\n",
    "# CSS selector for the specific ol element\n",
    "selector = \"#post-285 > div > ol:nth-child(2)\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the content of the request with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the specific ol element using the selector\n",
    "specific_ol = soup.select_one(selector)\n",
    "\n",
    "# Extract text from the ol element\n",
    "if specific_ol:\n",
    "    # Extract only the titles from list items\n",
    "    titles = [re.split(r'\\s*â€“\\s*', item.get_text(strip=True))[0] for item in specific_ol.find_all('li')]\n",
    "\n",
    "    # Join the titles into a single string\n",
    "    titles_text = '\\n'.join(titles)\n",
    "\n",
    "    # Write the extracted titles to '1000books.txt'\n",
    "    with open('1000books.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(titles_text)\n",
    "else:\n",
    "    print(\"Element not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Combine Best-Selling Books from Wikipedia Tables\n",
    "\n",
    "This code retrieves tables from a Wikipedia page containing information about best-selling books. The tables are read into Pandas DataFrames using the provided URL. The contents of the 'Book' column from these tables are then combined and written to a text file named 'topSellingBooks.txt'. The final output is a consolidated list of best-selling books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the files combined into 'topSellingBooks.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_best-selling_books#More_than_100_million_copies'\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Combine the contents of the files into a single file\n",
    "with open('topSellingBooks.txt', 'w', encoding='utf-8') as outfile:\n",
    "    for table in tables:\n",
    "        # If the table has a 'Book' column\n",
    "        if 'Book' in table.columns:\n",
    "            # Extract the 'Book' column\n",
    "            books = table['Book']\n",
    "            # Write the contents of the 'Book' column to the output file\n",
    "            outfile.write('\\n'.join(books))\n",
    "\n",
    "print(\"Contents of the files combined into 'topSellingBooks.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Connect to Neo4j database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to the Neo4j database\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))  # replace with your actual username and password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Import and Indexing for Neo4j Database\n",
    "\n",
    "### Query 1: Adding structured data to Neo4j DB\n",
    "This query loads data from a CSV file (`books.csv`) and creates nodes in Neo4j, setting properties based on the CSV data.\n",
    "\n",
    "### Query 2: Creating index on ISBN\n",
    "A Neo4j index is created on the 'ISBN' property of the 'Book' nodes to improve query performance.\n",
    "\n",
    "### Query 3: Importing semi-structured data into Neo4j\n",
    "Data from a CSV file (`goodreads_books_common.csv`) is iteratively imported into Neo4j, updating existing 'Book' nodes based on ISBN.\n",
    "\n",
    "### Query 4: Creating Index on 'Name' for faster runtime\n",
    "Indexes are created for 'Name' property of 'Book', 'name' property of 'Author', and 'count' property of 'Review'.\n",
    "\n",
    "### Query 5: Setting all values of best_seller to false\n",
    "This query iterates over all 'Book' nodes, setting the 'best_seller' and 'in_1000_collection' properties to false.\n",
    "\n",
    "### Query 6: Merging nodes with duplicate titles\n",
    "Nodes with duplicate 'Name' properties are merged, and extra nodes are detached from the database.\n",
    "\n",
    "### Set true queries\n",
    "Values are set to true for specific conditions using queries `qtrue` and `qtrue2`, based on book names from external text files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">702</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">18</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 702, errors: {}, committed: 702, failed: 0}</td><td style=\"text-align:left\">{total: 701789, errors: {}, committed: 701789, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 701789, labelsRemoved: 0, labelsAdded: 701789, propertiesSet: 12632201}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                               | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                                          \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|-----------------------------------------------------|-----------------------------------------------------------|---------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "     702 | 701789 |        18 |              701789 |                0 |             0 |       0 | {}            | {total: 702, errors: {}, committed: 702, failed: 0} | {total: 701789, errors: {}, committed: 701789, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 701789, labelsRemoved: 0, labelsAdded: 701789, propertiesSet: 12632201} "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 1: Adding structured data to Neo4j DB\n",
    "insertSD = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'LOAD CSV WITH HEADERS FROM \"file:///books.csv\" AS row RETURN row',\n",
    "  'CREATE (b:Book) SET b = row',\n",
    "  {batchSize:1000, iterateList:true, parallel:false}\n",
    ")\n",
    "\"\"\"\n",
    "graph.run(insertSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 2: Creating index on ISBN\n",
    "isbnIndex = \"\"\"\n",
    "CREATE INDEX FOR (b:Book) ON (b.ISBN)\n",
    "\"\"\"\n",
    "graph.run(isbnIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">228</td><td style=\"text-align:right\">227583</td><td style=\"text-align:right\">21</td><td style=\"text-align:right\">227583</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 228, errors: {}, committed: 228, failed: 0}</td><td style=\"text-align:left\">{total: 227583, errors: {}, committed: 227583, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 0, labelsRemoved: 0, labelsAdded: 0, propertiesSet: 4321808}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                               | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                               \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|-----------------------------------------------------|-----------------------------------------------------------|---------------|--------------|------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "     228 | 227583 |        21 |              227583 |                0 |             0 |       0 | {}            | {total: 228, errors: {}, committed: 228, failed: 0} | {total: 227583, errors: {}, committed: 227583, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 0, labelsRemoved: 0, labelsAdded: 0, propertiesSet: 4321808} "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 3: Importing semi-structured data into Neo4j\n",
    "insertSSD = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'LOAD CSV WITH HEADERS FROM \"file:///goodreads_books_common.csv\" AS row RETURN row',\n",
    "  'MATCH (b:Book {ISBN: row.isbn}) SET b += row',\n",
    "  {batchSize:1000, iterateList:true, parallel:false}\n",
    ")\n",
    "\"\"\"\n",
    "graph.run(insertSSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 4: Creating Index on 'Name' for faster runtime\n",
    "nameIndex = \"\"\"\n",
    "CREATE INDEX FOR (b:Book) ON (b.Name)\n",
    "\"\"\"\n",
    "graph.run(nameIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorIndex = \"\"\"\n",
    "CREATE INDEX FOR (a:Author) ON (a.name)\n",
    "\"\"\"\n",
    "graph.run(authorIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewCountIndex = \"\"\"\n",
    "CREATE INDEX FOR (r:Review) ON (r.count)\n",
    "\"\"\"\n",
    "graph.run(reviewCountIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">702</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">10</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 702, errors: {}, committed: 702, failed: 0}</td><td style=\"text-align:left\">{total: 701789, errors: {}, committed: 701789, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 0, labelsRemoved: 0, labelsAdded: 0, propertiesSet: 1403578}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                               | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                               \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|-----------------------------------------------------|-----------------------------------------------------------|---------------|--------------|------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "     702 | 701789 |        10 |              701789 |                0 |             0 |       0 | {}            | {total: 702, errors: {}, committed: 702, failed: 0} | {total: 701789, errors: {}, committed: 701789, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 0, labelsRemoved: 0, labelsAdded: 0, propertiesSet: 1403578} "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addUnstructuredProperties = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'MATCH (b:Book) RETURN b',\n",
    "  'SET b.best_seller = false, b.in_1000_collection = false',\n",
    "  {batchSize:1000, iterateList:true, parallel:false}\n",
    ")\n",
    "\"\"\"\n",
    "graph.run(addUnstructuredProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batches': 448,\n",
       "  'total': 44741,\n",
       "  'timeTaken': 3,\n",
       "  'committedOperations': 0,\n",
       "  'failedOperations': 44741,\n",
       "  'failedBatches': 448,\n",
       "  'retries': 0,\n",
       "  'errorMessages': {'Expected parameter(s): nodes': 448},\n",
       "  'batch': {'total': 448,\n",
       "   'errors': {'org.neo4j.graphdb.QueryExecutionException: Expected parameter(s): nodes': 448},\n",
       "   'committed': 0,\n",
       "   'failed': 448},\n",
       "  'operations': {'total': 44741,\n",
       "   'errors': {'Expected parameter(s): nodes': 448},\n",
       "   'committed': 0,\n",
       "   'failed': 44741},\n",
       "  'wasTerminated': False,\n",
       "  'failedParams': {},\n",
       "  'updateStatistics': {'relationshipsDeleted': 0,\n",
       "   'relationshipsCreated': 0,\n",
       "   'nodesDeleted': 0,\n",
       "   'nodesCreated': 0,\n",
       "   'labelsRemoved': 0,\n",
       "   'labelsAdded': 0,\n",
       "   'propertiesSet': 0}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicateRemoval = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'MATCH (b:Book)\n",
    "   WITH b.Name AS title, collect(b) AS nodes \n",
    "   WHERE size(nodes) > 1\n",
    "   RETURN nodes',\n",
    "  'FOREACH (ignored IN tail($nodes) | DETACH DELETE ignored)',\n",
    "  { batchSize: 100, parallel: false}\n",
    ");\n",
    "\"\"\"\n",
    "graph.run(duplicateRemoval).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">1</td><td style=\"text-align:right\">1000</td><td style=\"text-align:right\">104</td><td style=\"text-align:right\">1000</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 1, errors: {}, committed: 1, failed: 0}</td><td style=\"text-align:left\">{total: 1000, errors: {}, committed: 1000, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 0, labelsRemoved: 0, labelsAdded: 0, propertiesSet: 48944}</td></tr></table>"
      ],
      "text/plain": [
       " batches | total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                           | operations                                            | wasTerminated | failedParams | updateStatistics                                                                                                                             \n",
       "---------|-------|-----------|---------------------|------------------|---------------|---------|---------------|-------------------------------------------------|-------------------------------------------------------|---------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------\n",
       "       1 |  1000 |       104 |                1000 |                0 |             0 |       0 | {}            | {total: 1, errors: {}, committed: 1, failed: 0} | {total: 1000, errors: {}, committed: 1000, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 0, nodesDeleted: 0, nodesCreated: 0, labelsRemoved: 0, labelsAdded: 0, propertiesSet: 48944} "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from 'topSellingBooks.txt'\n",
    "with open('topSellingBooks.txt', 'r', encoding='utf-8') as file:\n",
    "    top_selling_books = [line.strip() for line in file]\n",
    "\n",
    "# Read data from 'thousandBooks.txt'\n",
    "with open('1000books.txt', 'r', encoding='utf-8') as file:\n",
    "    thousand_books = [line.strip() for line in file]\n",
    "    \n",
    "# Set values to true for specific conditions\n",
    "bestSellerTrue = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'UNWIND $names AS name RETURN name',\n",
    "  'MATCH (b:Book) WHERE b.Name CONTAINS name SET b.best_seller = true',\n",
    "  {batchSize:1000, params:{names: $top_selling_books}}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "thousandBookTrue = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  'UNWIND $names AS name RETURN name',\n",
    "  'MATCH (b:Book) WHERE b.Name CONTAINS name SET b.in_1000_collection = true',\n",
    "  {batchSize:1000, params:{names: $thousand_books}}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the queries\n",
    "graph.run(bestSellerTrue, top_selling_books=top_selling_books)\n",
    "graph.run(thousandBookTrue, thousand_books=thousand_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">71</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">10</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 71, errors: {}, committed: 71, failed: 0}</td><td style=\"text-align:left\">{total: 701789, errors: {}, committed: 701789, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 701789, nodesDeleted: 0, nodesCreated: 278663, labelsRemoved: 0, labelsAdded: 278663, propertiesSet: 278663}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                             | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                                             \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|---------------------------------------------------|-----------------------------------------------------------|---------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "      71 | 701789 |        10 |              701789 |                0 |             0 |       0 | {}            | {total: 71, errors: {}, committed: 71, failed: 0} | {total: 701789, errors: {}, committed: 701789, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 701789, nodesDeleted: 0, nodesCreated: 278663, labelsRemoved: 0, labelsAdded: 278663, propertiesSet: 278663} "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 9: Create WROTE relationships\n",
    "wroteRelationship = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "    \"MATCH (b:Book) UNWIND b.Authors AS author RETURN b, author\",\n",
    "    \"MERGE (a:Author {name: author}) MERGE (a)-[:WROTE]->(b)\",\n",
    "    {batchSize:10000, parallel:false})\n",
    "\"\"\"\n",
    "graph.run(wroteRelationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">228</td><td style=\"text-align:right\">227464</td><td style=\"text-align:right\">33</td><td style=\"text-align:right\">227464</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 228, errors: {}, committed: 228, failed: 0}</td><td style=\"text-align:left\">{total: 227464, errors: {}, committed: 227464, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 227464, nodesDeleted: 0, nodesCreated: 304, labelsRemoved: 0, labelsAdded: 304, propertiesSet: 304}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                               | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                                    \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|-----------------------------------------------------|-----------------------------------------------------------|---------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "     228 | 227464 |        33 |              227464 |                0 |             0 |       0 | {}            | {total: 228, errors: {}, committed: 228, failed: 0} | {total: 227464, errors: {}, committed: 227464, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 227464, nodesDeleted: 0, nodesCreated: 304, labelsRemoved: 0, labelsAdded: 304, propertiesSet: 304} "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 7: Create HAS_RATING relationships\n",
    "hasRatingRelationship = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  \"MATCH (b:Book) WHERE b.average_rating IS NOT NULL RETURN b\",\n",
    "  \"MERGE (r:Rating {value: b.average_rating}) MERGE (b)-[:HAS_RATING]->(r)\",\n",
    "  {batchSize: 10000, parallel: false}\n",
    ");\n",
    "\"\"\"\n",
    "graph.run(hasRatingRelationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">139</td><td style=\"text-align:right\">138680</td><td style=\"text-align:right\">5</td><td style=\"text-align:right\">138680</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 139, errors: {}, committed: 139, failed: 0}</td><td style=\"text-align:left\">{total: 138680, errors: {}, committed: 138680, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 138680, nodesDeleted: 0, nodesCreated: 81, labelsRemoved: 0, labelsAdded: 81, propertiesSet: 81}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                               | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                                 \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|-----------------------------------------------------|-----------------------------------------------------------|---------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "     139 | 138680 |         5 |              138680 |                0 |             0 |       0 | {}            | {total: 139, errors: {}, committed: 139, failed: 0} | {total: 138680, errors: {}, committed: 138680, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 138680, nodesDeleted: 0, nodesCreated: 81, labelsRemoved: 0, labelsAdded: 81, propertiesSet: 81} "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 8: Create WRITTEN_IN relationships\n",
    "writtenInRelationship = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  \"MATCH (b:Book) WHERE b.Language IS NOT NULL RETURN b\",\n",
    "  \"MERGE (l:Language {name: b.Language}) MERGE (b)-[:WRITTEN_IN]->(l)\",\n",
    "  {batchSize:10000, parallel:false}\n",
    ")\n",
    "\"\"\"\n",
    "graph.run(writtenInRelationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>batches</th><th>total</th><th>timeTaken</th><th>committedOperations</th><th>failedOperations</th><th>failedBatches</th><th>retries</th><th>errorMessages</th><th>batch</th><th>operations</th><th>wasTerminated</th><th>failedParams</th><th>updateStatistics</th></tr><tr><td style=\"text-align:right\">71</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">6</td><td style=\"text-align:right\">701789</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:right\">0</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{total: 71, errors: {}, committed: 71, failed: 0}</td><td style=\"text-align:left\">{total: 701789, errors: {}, committed: 701789, failed: 0}</td><td style=\"text-align:left\">false</td><td style=\"text-align:left\">{}</td><td style=\"text-align:left\">{relationshipsDeleted: 0, relationshipsCreated: 701789, nodesDeleted: 0, nodesCreated: 2679, labelsRemoved: 0, labelsAdded: 2679, propertiesSet: 2679}</td></tr></table>"
      ],
      "text/plain": [
       " batches |  total | timeTaken | committedOperations | failedOperations | failedBatches | retries | errorMessages | batch                                             | operations                                                | wasTerminated | failedParams | updateStatistics                                                                                                                                       \n",
       "---------|--------|-----------|---------------------|------------------|---------------|---------|---------------|---------------------------------------------------|-----------------------------------------------------------|---------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "      71 | 701789 |         6 |              701789 |                0 |             0 |       0 | {}            | {total: 71, errors: {}, committed: 71, failed: 0} | {total: 701789, errors: {}, committed: 701789, failed: 0} | false         | {}           | {relationshipsDeleted: 0, relationshipsCreated: 701789, nodesDeleted: 0, nodesCreated: 2679, labelsRemoved: 0, labelsAdded: 2679, propertiesSet: 2679} "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 10: Create HAS_REVIEW relationships\n",
    "hasReviewRelationship = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  \"MATCH (b:Book) RETURN b\",\n",
    "  \"MERGE (r:Review {count: toInteger(b.CountsOfReview)}) MERGE (b)-[:HAS_REVIEW]->(r)\",\n",
    "  {batchSize:10000, parallel:false}\n",
    ")\n",
    "\"\"\"\n",
    "graph.run(hasReviewRelationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 9: 5 Complex Queries \n",
    "\n",
    "### Query 1: Top Authors by Best Sellers\n",
    "Retrieves the top 15 authors with the highest number of best-selling books, displaying their names, all distinct books they wrote, and the total count of books.\n",
    "\n",
    "### Query 2: Authors with Diverse Language Writings\n",
    "Identifies authors who have written books in multiple languages, showcasing their names, distinct languages used, and the total number of books written.\n",
    "\n",
    "### Query 3: Authors with High Average Ratings\n",
    "Selects authors whose books have an average rating calculated from reviews with more than 10,000 ratings. Displays author names, average ratings, and total ratings counts.\n",
    "\n",
    "### Query 4: Best-Selling Books with Favorable Reviews\n",
    "Returns a list of book titles marked as best sellers with high ratings and favorable reviews, based on a 5 star rating distribution of over 50%. \n",
    "\n",
    "### Query 5: Books with the Most Similar Titles\n",
    "Retrieves titles of the top 75 books from the 'in_1000_collection' with the most similar titles, providing the count of similar titles and their respective work IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>AllBooks</th>\n",
       "      <th>BookCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anonymous</td>\n",
       "      <td>[Advice on the Art of Governance (Mau'izah-I J...</td>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Francine Pascal</td>\n",
       "      <td>[Wired (Fearless, #33), Love &amp; Betrayal &amp; Hold...</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harold Bloom</td>\n",
       "      <td>[Willa Cather (Bloom's Modern Critical Views),...</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carolyn Keene</td>\n",
       "      <td>[Dead on Arrival (Nancy Drew and the Hardy Boy...</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Isaac Asimov</td>\n",
       "      <td>[Henry Hudson: Arctic Explorer and North Ameri...</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Beowulf, When Were You Born, People of India,...</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R.L. Stine</td>\n",
       "      <td>[The Beast from the East (Goosebumps, #43), Su...</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Agatha Christie</td>\n",
       "      <td>[Dame Agatha Abroad: Murder on the Orient Expr...</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hal Leonard Publishing Company</td>\n",
       "      <td>[Eric Clapton - From the Cradle, The Music Man...</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NOT A BOOK</td>\n",
       "      <td>[Red Covers Postcard Book, Radio Program:  The...</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>[Letters from Hawaii, The Adventures of Tom Sa...</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Various</td>\n",
       "      <td>[Mit 66 Jahren, da fÃ¤ngt das Morden an., Afric...</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Franklin W. Dixon</td>\n",
       "      <td>[Spy Set (Hardy Boys: Undercover Brothers, #1-...</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>['Salem's Lot, The Green Mile, Christine, Anot...</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Enid Blyton</td>\n",
       "      <td>[Noddy's Super Busy Day, The Secret Seven &amp; Se...</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Author  \\\n",
       "0                        Anonymous   \n",
       "1                  Francine Pascal   \n",
       "2                     Harold Bloom   \n",
       "3                    Carolyn Keene   \n",
       "4                     Isaac Asimov   \n",
       "5                          Unknown   \n",
       "6                       R.L. Stine   \n",
       "7                  Agatha Christie   \n",
       "8   Hal Leonard Publishing Company   \n",
       "9                       NOT A BOOK   \n",
       "10                      Mark Twain   \n",
       "11                         Various   \n",
       "12               Franklin W. Dixon   \n",
       "13                    Stephen King   \n",
       "14                     Enid Blyton   \n",
       "\n",
       "                                             AllBooks  BookCount  \n",
       "0   [Advice on the Art of Governance (Mau'izah-I J...       1159  \n",
       "1   [Wired (Fearless, #33), Love & Betrayal & Hold...        622  \n",
       "2   [Willa Cather (Bloom's Modern Critical Views),...        509  \n",
       "3   [Dead on Arrival (Nancy Drew and the Hardy Boy...        442  \n",
       "4   [Henry Hudson: Arctic Explorer and North Ameri...        425  \n",
       "5   [Beowulf, When Were You Born, People of India,...        400  \n",
       "6   [The Beast from the East (Goosebumps, #43), Su...        397  \n",
       "7   [Dame Agatha Abroad: Murder on the Orient Expr...        311  \n",
       "8   [Eric Clapton - From the Cradle, The Music Man...        298  \n",
       "9   [Red Covers Postcard Book, Radio Program:  The...        243  \n",
       "10  [Letters from Hawaii, The Adventures of Tom Sa...        235  \n",
       "11  [Mit 66 Jahren, da fÃ¤ngt das Morden an., Afric...        227  \n",
       "12  [Spy Set (Hardy Boys: Undercover Brothers, #1-...        226  \n",
       "13  ['Salem's Lot, The Green Mile, Christine, Anot...        220  \n",
       "14  [Noddy's Super Busy Day, The Secret Seven & Se...        201  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 1\n",
    "query1 = \"\"\"\n",
    "MATCH (author:Author)-[:WROTE]->(book:Book)\n",
    "WITH author, COLLECT(DISTINCT book.Name) AS AllBooks, ANY(b IN COLLECT(book) WHERE b.best_seller = true) AS hasBestSeller\n",
    "WHERE hasBestSeller\n",
    "RETURN author.name AS Author, AllBooks, SIZE(AllBooks) AS BookCount\n",
    "ORDER BY BookCount DESC\n",
    "LIMIT 15;\n",
    "\"\"\"\n",
    "result1 = graph.run(query1)\n",
    "result1.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Languages</th>\n",
       "      <th>NumberOfBooks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Francine Pascal</td>\n",
       "      <td>[eng, en-US, spa, en-GB]</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nora Roberts</td>\n",
       "      <td>[eng, en-US, fre, spa, en-GB, ger]</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>[eng, en-US, fre, spa, en-GB, en-CA, ger, ita,...</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous</td>\n",
       "      <td>[eng, en-US, fre, spa, mul, en-GB, grc, enm, j...</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agatha Christie</td>\n",
       "      <td>[eng, en-US, fre, spa, en-GB, ger, ita]</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>[eng, en-US, fre, spa, mul, en-GB, enm, ger]</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R.L. Stine</td>\n",
       "      <td>[eng, en-US, fre, spa, ger, ita]</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ann M. Martin</td>\n",
       "      <td>[eng, en-US, fre, spa, en-GB]</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Terry Pratchett</td>\n",
       "      <td>[eng, en-US, fre, spa, en-GB, en-CA, ger]</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Isaac Asimov</td>\n",
       "      <td>[eng, en-US, fre, spa, en-GB, ger]</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Author                                          Languages  \\\n",
       "0      Francine Pascal                           [eng, en-US, spa, en-GB]   \n",
       "1         Nora Roberts                 [eng, en-US, fre, spa, en-GB, ger]   \n",
       "2         Stephen King  [eng, en-US, fre, spa, en-GB, en-CA, ger, ita,...   \n",
       "3            Anonymous  [eng, en-US, fre, spa, mul, en-GB, grc, enm, j...   \n",
       "4      Agatha Christie            [eng, en-US, fre, spa, en-GB, ger, ita]   \n",
       "5  William Shakespeare       [eng, en-US, fre, spa, mul, en-GB, enm, ger]   \n",
       "6           R.L. Stine                   [eng, en-US, fre, spa, ger, ita]   \n",
       "7        Ann M. Martin                      [eng, en-US, fre, spa, en-GB]   \n",
       "8      Terry Pratchett          [eng, en-US, fre, spa, en-GB, en-CA, ger]   \n",
       "9         Isaac Asimov                 [eng, en-US, fre, spa, en-GB, ger]   \n",
       "\n",
       "   NumberOfBooks  \n",
       "0            412  \n",
       "1            373  \n",
       "2            367  \n",
       "3            292  \n",
       "4            290  \n",
       "5            268  \n",
       "6            252  \n",
       "7            248  \n",
       "8            196  \n",
       "9            191  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 2\n",
    "query2 = \"\"\"\n",
    "MATCH (a:Author)-[:WROTE]->(b:Book)-[:WRITTEN_IN]->(l:Language)\n",
    "WITH distinct(a), COLLECT(DISTINCT l.name) AS Languages, COUNT(*) AS NumberOfBooks\n",
    "WHERE size(Languages) > 1\n",
    "RETURN distinct(a.name) AS Author, Languages, NumberOfBooks\n",
    "ORDER BY NumberOfBooks DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "result2 = graph.run(query2)\n",
    "result2.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>total_ratings_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Watterson</td>\n",
       "      <td>4.724286</td>\n",
       "      <td>167309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anonymous</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>56209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gordon B. Hinckley</td>\n",
       "      <td>4.610000</td>\n",
       "      <td>12811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alisa Kwitney</td>\n",
       "      <td>4.610000</td>\n",
       "      <td>11702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gary Russell</td>\n",
       "      <td>4.590000</td>\n",
       "      <td>24419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jude Fisher</td>\n",
       "      <td>4.590000</td>\n",
       "      <td>17516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.574615</td>\n",
       "      <td>5827771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Patrick Rothfuss</td>\n",
       "      <td>4.570000</td>\n",
       "      <td>253462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sarah  Young</td>\n",
       "      <td>4.540000</td>\n",
       "      <td>35891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiromu Arakawa</td>\n",
       "      <td>4.535000</td>\n",
       "      <td>131216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chris   Smith</td>\n",
       "      <td>4.530000</td>\n",
       "      <td>18846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Angela Elwell Hunt</td>\n",
       "      <td>4.520000</td>\n",
       "      <td>13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Patricia Polacco</td>\n",
       "      <td>4.520000</td>\n",
       "      <td>13390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Founding Fathers</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>31479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Harold McGee</td>\n",
       "      <td>4.480000</td>\n",
       "      <td>10774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Author  avg_rating  total_ratings_count\n",
       "0       Bill Watterson    4.724286               167309\n",
       "1            Anonymous    4.640000                56209\n",
       "2   Gordon B. Hinckley    4.610000                12811\n",
       "3        Alisa Kwitney    4.610000                11702\n",
       "4         Gary Russell    4.590000                24419\n",
       "5          Jude Fisher    4.590000                17516\n",
       "6         J.K. Rowling    4.574615              5827771\n",
       "7     Patrick Rothfuss    4.570000               253462\n",
       "8         Sarah  Young    4.540000                35891\n",
       "9       Hiromu Arakawa    4.535000               131216\n",
       "10       Chris   Smith    4.530000                18846\n",
       "11  Angela Elwell Hunt    4.520000                13410\n",
       "12    Patricia Polacco    4.520000                13390\n",
       "13    Founding Fathers    4.490000                31479\n",
       "14        Harold McGee    4.480000                10774"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 3\n",
    "query3 = \"\"\"\n",
    "MATCH (a:Author)-[:WROTE]->(b:Book)-[:HAS_RATING]->(r:Rating)\n",
    "WHERE toInteger(b.ratings_count) > 10000\n",
    "WITH a, COLLECT({rating: toFloat(r.value), ratingsCount: toInteger(b.ratings_count)}) AS ratingsData\n",
    "RETURN\n",
    "a.name AS Author,\n",
    "REDUCE(s = 0.0, r IN ratingsData | s + r.rating) / SIZE(ratingsData) AS avg_rating,\n",
    "REDUCE(totalCount = 0, r IN ratingsData | totalCount + r.ratingsCount) AS total_ratings_count\n",
    "ORDER BY avg_rating DESC\n",
    "LIMIT 15;\n",
    "\"\"\"\n",
    "result3 = graph.run(query3)\n",
    "result3.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>BestSeller</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hatchet (Brian's Saga, #1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atlas Shrugged</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Of Mice and Men</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Angels &amp; Demons (Robert Langdon, #1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Da Vinci Code (Robert Langdon, #2)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Snow Flower and the Secret Fan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Freakonomics: A Rogue Economist Explores the H...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A Million Little Pieces</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Night  (The Night Trilogy #1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Curious Incident of the Dog in the Night-Time</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Into the Wild</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Into Thin Air: A Personal Account of the Mount...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Little Women</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Persuasion</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Middlesex</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Tipping Point: How Little Things Can Make ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Adventures of Huckleberry Finn</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Five People You Meet in Heaven</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A Walk to Remember</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Giver (The Giver, #1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Man's Search for Meaning</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>American Gods (American Gods, #1)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Extremely Loud &amp; Incredibly Close</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>How to Win Friends and Influence People</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Kafka on the Shore</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Slaughterhouse-Five</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The Red Tent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The Pillars of the Earth (Kingsbridge, #1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Catcher in the Rye</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Brave New World</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The Picture of Dorian Gray</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A Christmas Carol</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Dragonfly in Amber (Outlander, #2)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1984</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Bel Canto</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Hobbit, or There and Back Again</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>The Road</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Never Let Me Go</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Emma</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>The Glass Castle</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Anne of Green Gables (Anne of Green Gables, #1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>World War Z: An Oral History of the Zombie War</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  BestSeller\n",
       "0   Harry Potter and the Half-Blood Prince (Harry ...        True\n",
       "1   Harry Potter and the Order of the Phoenix (Har...        True\n",
       "2   Harry Potter and the Sorcerer's Stone (Harry P...       False\n",
       "3   Harry Potter and the Prisoner of Azkaban (Harr...        True\n",
       "4   Harry Potter and the Goblet of Fire (Harry Pot...        True\n",
       "5   The Fellowship of the Ring (The Lord of the Ri...       False\n",
       "6                          Hatchet (Brian's Saga, #1)       False\n",
       "7                                      Atlas Shrugged       False\n",
       "8                                     Of Mice and Men       False\n",
       "9                Angels & Demons (Robert Langdon, #1)        True\n",
       "10             The Da Vinci Code (Robert Langdon, #2)        True\n",
       "11                     Snow Flower and the Secret Fan       False\n",
       "12  Freakonomics: A Rogue Economist Explores the H...       False\n",
       "13                            A Million Little Pieces       False\n",
       "14                      Night  (The Night Trilogy #1)       False\n",
       "15  The Curious Incident of the Dog in the Night-Time       False\n",
       "16                                      Into the Wild       False\n",
       "17                                Pride and Prejudice        True\n",
       "18  Into Thin Air: A Personal Account of the Mount...       False\n",
       "19                                       Little Women       False\n",
       "20                                         Persuasion       False\n",
       "21                                          Middlesex       False\n",
       "22  The Tipping Point: How Little Things Can Make ...       False\n",
       "23                 The Adventures of Huckleberry Finn        True\n",
       "24                 The Five People You Meet in Heaven       False\n",
       "25                                 A Walk to Remember       False\n",
       "26                          The Giver (The Giver, #1)        True\n",
       "27                           Man's Search for Meaning       False\n",
       "28                  American Gods (American Gods, #1)       False\n",
       "29                  Extremely Loud & Incredibly Close       False\n",
       "30            How to Win Friends and Influence People        True\n",
       "31                                 Kafka on the Shore       False\n",
       "32                                Slaughterhouse-Five       False\n",
       "33                                       The Red Tent       False\n",
       "34         The Pillars of the Earth (Kingsbridge, #1)        True\n",
       "35                             The Catcher in the Rye        True\n",
       "36                                    Brave New World       False\n",
       "37                         The Picture of Dorian Gray       False\n",
       "38                                  A Christmas Carol       False\n",
       "39                 Dragonfly in Amber (Outlander, #2)       False\n",
       "40                                               1984       False\n",
       "41                                          Bel Canto       False\n",
       "42                The Hobbit, or There and Back Again       False\n",
       "43                                           The Road       False\n",
       "44                                    Never Let Me Go       False\n",
       "45                                               Emma       False\n",
       "46                                   The Glass Castle       False\n",
       "47                                        Animal Farm       False\n",
       "48    Anne of Green Gables (Anne of Green Gables, #1)        True\n",
       "49     World War Z: An Oral History of the Zombie War       False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 4\n",
    "query4 = \"\"\"\n",
    "MATCH (b:Book)-[:HAS_REVIEW]->(r:Review)\n",
    "WHERE toInteger(r.count) > 10000 AND toInteger(split(b.RatingDist5, ':')[1]) > 0.5 * toInteger(r.count)\n",
    "RETURN b.Name AS Title, b.best_seller AS BestSeller\n",
    "LIMIT 50;\n",
    "\"\"\"\n",
    "result4 = graph.run(query4)\n",
    "result4.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>NumberOfSimilarBooks</th>\n",
       "      <th>SimilarBookWorkIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Short History of Nearly Everything</td>\n",
       "      <td>18</td>\n",
       "      <td>[['373572',  '6117055',  '331227',  '2018682',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bryson's Dictionary of Troublesome Words: A Wr...</td>\n",
       "      <td>18</td>\n",
       "      <td>[['216487',  '700653',  '275815',  '138270',  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hatchet (Brian's Saga, #1)</td>\n",
       "      <td>18</td>\n",
       "      <td>[['41667',  '438131',  '124245',  '13164526', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guts: The True Stories behind Hatchet and the ...</td>\n",
       "      <td>18</td>\n",
       "      <td>[['295419',  '175089',  '611726',  '295617',  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Heidi Chronicles: Uncommon Women and Other...</td>\n",
       "      <td>18</td>\n",
       "      <td>[['205805',  '413705',  '784709',  '170539',  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>To the Lighthouse</td>\n",
       "      <td>18</td>\n",
       "      <td>[['31072',  '91494',  '366524',  '126583',  '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Mrs. Dalloway</td>\n",
       "      <td>18</td>\n",
       "      <td>[['160010',  '16810',  '3102',  '606805',  '51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>A Room of One's Own</td>\n",
       "      <td>18</td>\n",
       "      <td>[['149709',  '224387',  '85767',  '98532',  '6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>The Passion</td>\n",
       "      <td>18</td>\n",
       "      <td>[['258627',  '304157',  '31186',  '115009',  '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Easy Riders, Raging Bulls: How the Sex-Drugs-A...</td>\n",
       "      <td>18</td>\n",
       "      <td>[['2092154',  '542635',  '641355',  '459744', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  NumberOfSimilarBooks  \\\n",
       "0                A Short History of Nearly Everything                    18   \n",
       "1   Bryson's Dictionary of Troublesome Words: A Wr...                    18   \n",
       "2                          Hatchet (Brian's Saga, #1)                    18   \n",
       "3   Guts: The True Stories behind Hatchet and the ...                    18   \n",
       "4   The Heidi Chronicles: Uncommon Women and Other...                    18   \n",
       "..                                                ...                   ...   \n",
       "70                                  To the Lighthouse                    18   \n",
       "71                                      Mrs. Dalloway                    18   \n",
       "72                                A Room of One's Own                    18   \n",
       "73                                        The Passion                    18   \n",
       "74  Easy Riders, Raging Bulls: How the Sex-Drugs-A...                    18   \n",
       "\n",
       "                                   SimilarBookWorkIds  \n",
       "0   [['373572',  '6117055',  '331227',  '2018682',...  \n",
       "1   [['216487',  '700653',  '275815',  '138270',  ...  \n",
       "2   [['41667',  '438131',  '124245',  '13164526', ...  \n",
       "3   [['295419',  '175089',  '611726',  '295617',  ...  \n",
       "4   [['205805',  '413705',  '784709',  '170539',  ...  \n",
       "..                                                ...  \n",
       "70  [['31072',  '91494',  '366524',  '126583',  '1...  \n",
       "71  [['160010',  '16810',  '3102',  '606805',  '51...  \n",
       "72  [['149709',  '224387',  '85767',  '98532',  '6...  \n",
       "73  [['258627',  '304157',  '31186',  '115009',  '...  \n",
       "74  [['2092154',  '542635',  '641355',  '459744', ...  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query 5\n",
    "query5 = \"\"\"\n",
    "MATCH (b:Book)\n",
    "WHERE b.in_1000_collection = true\n",
    "WITH b, size(split(coalesce(b.similar_books, ''), ',')) AS NumberOfSimilarBooks, split(coalesce(b.similar_books, ''), ',') AS SimilarBookWorkIds\n",
    "OPTIONAL MATCH (b2:Book)\n",
    "WHERE b2.work_id IN SimilarBookWorkIds\n",
    "RETURN b.Name AS Title, NumberOfSimilarBooks, SimilarBookWorkIds\n",
    "ORDER BY NumberOfSimilarBooks DESC\n",
    "LIMIT 75;\n",
    "\"\"\"\n",
    "result5 = graph.run(query5)\n",
    "result5.to_data_frame()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
